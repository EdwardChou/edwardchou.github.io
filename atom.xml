<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Edward</title>
  <subtitle>Technology Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://edwardchou.github.io/"/>
  <updated>2016-10-20T07:40:31.729Z</updated>
  <id>http://edwardchou.github.io/</id>
  
  <author>
    <name>Edward Chou</name>
    <email>edwardchouxz@foxmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习优化方法汇总比较</title>
    <link href="http://edwardchou.github.io/2016/10/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB%E6%AF%94%E8%BE%83/"/>
    <id>http://edwardchou.github.io/2016/10/20/深度学习优化方法汇总比较/</id>
    <published>2016-10-20T07:04:12.000Z</published>
    <updated>2016-10-20T07:40:31.729Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>（标题不能再中二了）本文仅对一些常见的优化方法进行直观介绍和简单的比较，各种优化方法的详细内容及公式只好去认真啃论文了，在此我就不赘述了。</p>
<h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>此处的SGD指mini-batch gradient descent，关于batch gradient descent, stochastic gradient descent, 以及 mini-batch gradient descent的具体区别就不细说了。现在的SGD一般都指mini-batch gradient descent。</p>
<p>SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即：</p>
<p>其中，是学习率，是梯度 SGD完全依赖于当前batch的梯度，所以可理解为允许当前batch的梯度多大程度影响参数更新</p>
<p>缺点：（正因为有这些缺点才让这么多大神发展出了后续的各种算法）</p>
<ul>
<li>选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了</li>
<li>SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点【原来写的是“容易困于鞍点”，经查阅论文发现，其实在合适的初始化和step size的情况下，鞍点的影响并没这么大。感谢@冰橙的指正】</li>
</ul>
<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。公式如下：</p>
<p>其中，是动量因子</p>
<p>特点：</p>
<ul>
<li>下降初期时，使用上一次参数更新，下降方向一致，乘上较大的能够进行很好的加速</li>
<li>下降中后期时，在局部最小值来回震荡的时候，，使得更新幅度增大，跳出陷阱</li>
<li>在梯度改变方向的时候，能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛</li>
</ul>
<h2 id="Nesterov"><a href="#Nesterov" class="headerlink" title="Nesterov"></a>Nesterov</h2><p>nesterov项在梯度更新时做一个校正，避免前进太快，同时提高灵敏度。 将上一节中的公式展开可得：</p>
<p>可以看出，并没有直接改变当前梯度，所以Nesterov的改进就是让之前的动量直接影响当前的动量。即：</p>
<p>所以，加上nesterov项后，梯度在大的跳跃后，进行计算对当前梯度进行校正。如下图：</p>
<p>momentum首先计算一个梯度(短的蓝色向量)，然后在加速更新梯度的方向进行一个大的跳跃(长的蓝色向量)，nesterov项首先在之前加速的梯度方向进行一个大的跳跃(棕色向量)，计算梯度然后进行校正(绿色梯向量)</p>
<p>其实，momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。但是，人工设置一些学习率总还是有些生硬，接下来介绍几种自适应学习率的方法</p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>Adagrad其实是对学习率进行了一个约束。即：</p>
<p>此处，对从1到进行一个递推形成一个约束项regularizer，，用来保证分母非0</p>
<p>特点：</p>
<ul>
<li>前期较小的时候， regularizer较大，能够放大梯度</li>
<li>后期较大的时候，regularizer较小，能够约束梯度</li>
<li>适合处理稀疏梯度<br>缺点：</li>
<li>由公式可以看出，仍依赖于人工设置一个全局学习率</li>
<li>设置过大的话，会使regularizer过于敏感，对梯度的调节太大</li>
<li>中后期，分母上梯度平方的累加将会越来越大，使，使得训练提前结束</li>
</ul>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。 Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。即：</p>
<p>在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后：</p>
<p>其中，代表求期望。</p>
<p>此时，可以看出Adadelta已经不用依赖于全局学习率了。</p>
<p>特点：</p>
<p>训练初中期，加速效果不错，很快<br>训练后期，反复在局部最小值附近抖动</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop可以算作Adadelta的一个特例：</p>
<p>当时，就变为了求梯度平方和的平均数。</p>
<p>如果再求根的话，就变成了RMS(均方根)：</p>
<p>此时，这个RMS就可以作为学习率的一个约束：</p>
<p>特点：</p>
<p>其实RMSprop依然依赖于全局学习率<br>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间<br>适合处理非平稳目标 - 对于RNN效果很好</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下：</p>
<p>其中，，分别是对梯度的一阶矩估计和二阶矩估计，可以看作对期望，的估计；，是对，的校正，这样可以近似为对期望的无偏估计。 可以看出，直接对梯度的矩估计对内存没有额外的要求，而且可以根据梯度进行动态调整，而对学习率形成一个动态约束，而且有明确的范围。</p>
<p>特点：</p>
<ul>
<li>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li>
<li>对内存需求较小</li>
<li>为不同的参数计算不同的自适应学习率</li>
<li>也适用于大多非凸优化 - 适用于大数据集和高维空间</li>
</ul>
<h2 id="Adamax"><a href="#Adamax" class="headerlink" title="Adamax"></a>Adamax</h2><p>Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。公式上的变化如下：</p>
<p>可以看出，Adamax学习率的边界范围更简单</p>
<h2 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h2><p>Nadam类似于带有Nesterov动量项的Adam。公式如下：</p>
<p>可以看出，Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果。</p>
<h2 id="经验之谈"><a href="#经验之谈" class="headerlink" title="经验之谈"></a>经验之谈</h2><ul>
<li>对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值</li>
<li>SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠</li>
<li>如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。</li>
<li>Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。</li>
<li>在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果</li>
</ul>
<p>最后展示两张可厉害的图，一切尽在图中啊，上面的都没啥用了… …<br>损失平面等高线</p>
<p>在鞍点处的比较</p>
<p>转载须全文转载且注明作者和原文链接，否则保留维权权利</p>
<p>引用</p>
<p>[1]<a href="https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank" rel="external">Adagrad</a><br>[2]<a href="https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Etijmen/csc321/lecture_notes.shtml" target="_blank" rel="external">RMSprop[Lecture 6e]</a><br>[3]<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1212.5701" target="_blank" rel="external">Adadelta</a><br>[4]<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1412.6980v8" target="_blank" rel="external">Adam</a><br>[5]<a href="http://159.226.251.230/videoplayer/054_report.pdf?ich_u_r_i=e3605a9c53911b35bd50072bcace1bfa&amp;ich_s_t_a_r_t=0&amp;ich_e_n_d=0&amp;ich_k_e_y=1645108920750663512480&amp;ich_t_y_p_e=1&amp;ich_d_i_s_k_i_d=5&amp;ich_u_n_i_t=1" target="_blank" rel="external">Nadam</a><br>[6]<a href="https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Efritz/absps/momentum.pdf" target="_blank" rel="external">On the importance of initialization and momentum in deep learning</a><br>[7]<a href="https://link.zhihu.com/?target=http%3A//keras-cn.readthedocs.io/en/latest/" target="_blank" rel="external">Keras中文文档</a><br>[8]<a href="https://link.zhihu.com/?target=https%3A//twitter.com/alecrad" target="_blank" rel="external">Alec Radford(图)</a><br>[9]<a href="https://link.zhihu.com/?target=http%3A//sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">An overview of gradient descent optimization algorithms</a><br>[10]<a href="https://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v49/lee16.pdf" target="_blank" rel="external">Gradient Descent Only Converges to Minimizers</a><br>[11]<a href="https://link.zhihu.com/?target=http%3A//www.nature.com/nature/journal/v521/n7553/abs/nature14539.html" target="_blank" rel="external">Deep Learning:Nature</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;（标题不能再中二了）本文仅对一些常见的优化方法进行直观介绍和简单的比较，各种优化方法的详细内容及公式只好去认真啃论文了，在此我就不赘述了。&lt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://edwardchou.github.io/2016/07/20/hello-world/"/>
    <id>http://edwardchou.github.io/2016/07/20/hello-world/</id>
    <published>2016-07-20T13:58:14.650Z</published>
    <updated>2016-07-20T13:58:14.650Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
