{"meta":{"title":"Edward","subtitle":"CV,Android,Face Recognition","description":"Android,Caffe,Face Recognition","author":"Edward Chou","url":"http://edwardchou.github.io"},"pages":[{"title":"tags","date":"2016-07-20T14:34:29.000Z","updated":"2016-07-20T14:35:09.062Z","comments":false,"path":"tags/index.html","permalink":"http://edwardchou.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"人脸识别-中心损失函数","slug":"人脸识别-中心损失函数","date":"2016-10-20T09:06:31.000Z","updated":"2016-10-20T09:13:47.511Z","comments":true,"path":"2016/10/20/人脸识别-中心损失函数/","link":"","permalink":"http://edwardchou.github.io/2016/10/20/人脸识别-中心损失函数/","excerpt":"","text":"本论文对人脸识别和验证任务提出一种新的损失函数，即中心损失。中心损失和softmax损失联合监督学习的CNN，其对深层学习特征的人脸识别能力大大提高。对几个&gt;大型人脸基准的实验已经令人信服地证明了该方法的有效性。 相关论文题目：A Discriminative Feature Learning Approachfor Deep Face Recognition 作者：Yandong Wen, Kaipeng Zhang, Zhifeng Li*, YuQiao 论文摘要卷积神经网络（CNN）已经广泛应用于计算机视觉领域，显著提高了现有计算机视觉水平。在大多数可用的CNN中，softmax损失函数被用作训练深度模型的监视信号。为了增强深度学习特征的识别能力，本论文为人脸识别任务提出一种新的监视信号，称作中心损失函数（center loss）。具体地说，中心损失函数学习每类数据的深层特征的中心，同时惩罚深层特征和它们对应的类中心间的距离。更重要的是，我们证明了这种中心损失函数是可训练的，而且在CNN中非常容易优化。 通过Softmax损失函数和中心损失函数的联合监视，我们可以训练足够强大的CNN，得到两个关键学习目标的深层特征以及尽可能高的类间分散性和类内紧密性，这对人脸识别来说非常重要。令人鼓舞的是，我们这种联合监视的CNN在几个重要人脸识别基准上取得了最高的准确率，包括Labeled Faces inthe Wild (LFW)，YouTube Faces (YTF)，和MegaFace Challenge。尤其是，我们的新方法在小型训练集（少于500000幅图像、少于20000个人）协议下的MegaFace（最大的公共领域基准）上得到了最好结果，相比以&gt;前的结果有显著的提高，这为人脸识别和人脸验证任务带来新的发展。 CNN架构图：人脸识别任务中使用的CNN架构（来自论文，下同） 用LFW和YTF数据集实验LFW和YTF中的部分人脸图像，绿色框是同个人，红色反之，白色框的人脸用于测试： 实验结果实验中，model A是单一使用softmax损失函数监视的模型，model B是softmax损失和对比损失联合监视的模型，model C是softmax损失和中心损失联合监视的模型。实&gt;验结果显示，model C的性能比model A和modelB性能更好，在LFW和YTF中都能得到更高的准确率。 用MegaFaceChallenge数据集实验 MegaFace中的人脸图像样本：我们对使用不同方法的模型进行了人脸识别和人脸验证实验，结果如下：人脸识别任务中不同模型的正确率人脸验证任务中不同模型的正确率 结果显示，使用 softmax 损失函数和中心损失函数联合监视的 model C 能得到更高的正确率。 总结本论文中我们对人脸识别和验证任务提出一种新的损失函数，即中心损失。中心损失和softmax损失联合监督学习的CNN，其对深层学习特征的人脸识别能力大大提高。&gt;对几个大型人脸基准的实验已经令人信服地证明了该方法的有效性。 开源这篇论文发表于2016欧洲计算机视觉大会（ECCV 2016），论文作者于10月12日和13日开源了训练模型和提取深层特征demo。 开源地址：https://github.com/ydwen/caffe-face 训练模型 1.安装Caffe，请按照安装指南进行，确保在使用我们的代码前已经正确安装caffe。 2.下载人脸数据集，e.g. CAISA-WebFace,VGG-Face, MS-Celeb-1M, MegaFace. 3.预处理训练人脸图像，包括检测、对齐等。这里我们强烈推荐MTCNN，它是用于人脸识别和对齐非常高效的开源工具。 4.创建训练集和验证集的列表，放到face_example /data / 5.指定train和val的数据源 123456789layer &#123; name: &quot;data&quot; type: &quot;ImageData&quot; top: &quot;data&quot; top: &quot;label&quot; image_data_param &#123; source: &quot;face_example/data/caisa_train.txt&quot; &#125; &#125; 6.指定FC6层的目标数量 1234567layer &#123; name: &quot;fc6&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc5&quot; top: &quot;fc6&quot; inner_product_param &#123; num_output: 10572 &#125; 7.指定中心损失层目标数量和损失权重 1234567891011layer &#123; name: &quot;center_loss&quot; type: &quot;CenterLoss&quot; bottom: &quot;fc5&quot; bottom: &quot;label&quot; top: &quot;center_loss&quot; center_loss_param &#123; num_output: 10572 &#125; loss_weight: 0.008&#125; 8.训练模型 提取深层特征 1.编译及配置matcaffe 2.在face_example/extractDeepFeature.m中指定相应的路径 1234addpath('path_to_matCaffe/matlab')model = 'path_to_deploy/face_deploy.prototxt';weights = 'path_to_model/face_model.caffemodel';image = imread('path_to_image/Jennifer_Aniston_0003.jpg') 3.在Matlab运行extractDeepFeature.m-","categories":[],"tags":[]},{"title":"深度学习优化方法汇总比较","slug":"深度学习优化方法汇总比较","date":"2016-10-20T07:04:12.000Z","updated":"2016-10-20T08:27:28.041Z","comments":true,"path":"2016/10/20/深度学习优化方法汇总比较/","link":"","permalink":"http://edwardchou.github.io/2016/10/20/深度学习优化方法汇总比较/","excerpt":"","text":"前言（标题不能再中二了）本文仅对一些常见的优化方法进行直观介绍和简单的比较，各种优化方法的详细内容及公式只好去认真啃论文了，在此我就不赘述了。 SGD此处的SGD指mini-batch gradient descent，关于batch gradient descent, stochastic gradient descent, 以及 mini-batch gradient descent的具体区别就不细说了。现在的SGD一般都指mini-batch gradient descent。 SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即： 其中，是学习率，是梯度 SGD完全依赖于当前batch的梯度，所以可理解为允许当前batch的梯度多大程度影响参数更新 缺点：（正因为有这些缺点才让这么多大神发展出了后续的各种算法） 选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了 SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点【原来写的是“容易困于鞍点”，经查阅论文发现，其实在合适的初始化和step size的情况下，鞍点的影响并没这么大。感谢@冰橙的指正】 Momentummomentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。公式如下： 其中，μ是动量因子 特点： 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的μ能够进行很好的加速 下降中后期时，在局部最小值来回震荡的时候，，μ使得更新幅度增大，跳出陷阱 在梯度改变方向的时候，μ能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛 Nesterovnesterov项在梯度更新时做一个校正，避免前进太快，同时提高灵敏度。 将上一节中的公式展开可得：可以看出，并没有直接改变当前梯度，所以Nesterov的改进就是让之前的动量直接影响当前的动量。即： 所以，加上nesterov项后，梯度在大的跳跃后，进行计算对当前梯度进行校正。如下图： momentum首先计算一个梯度(短的蓝色向量)，然后在加速更新梯度的方向进行一个大的跳跃(长的蓝色向量)，nesterov项首先在之前加速的梯度方向进行一个大的跳跃(棕色向量)，计算梯度然后进行校正(绿色梯向量) 其实，momentum项和nesterov项都是为了使梯度更新更加灵活，对不同情况有针对性。但是，人工设置一些学习率总还是有些生硬，接下来介绍几种自适应学习率的方法 AdagradAdagrad其实是对学习率进行了一个约束。即： 此处，对从1到进行一个递推形成一个约束项regularizer，，用来保证分母非0 特点： 前期较小的时候， regularizer较大，能够放大梯度 后期较大的时候，regularizer较小，能够约束梯度 适合处理稀疏梯度缺点： 由公式可以看出，仍依赖于人工设置一个全局学习率 设置过大的话，会使regularizer过于敏感，对梯度的调节太大 中后期，分母上梯度平方的累加将会越来越大，使，使得训练提前结束 AdadeltaAdadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。 Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。即： 在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后： 其中，代表求期望。 此时，可以看出Adadelta已经不用依赖于全局学习率了。 特点： 训练初中期，加速效果不错，很快训练后期，反复在局部最小值附近抖动 RMSpropRMSprop可以算作Adadelta的一个特例： 当时，就变为了求梯度平方和的平均数。 如果再求根的话，就变成了RMS(均方根)： 此时，这个RMS就可以作为学习率的一个约束： 特点： 其实RMSprop依然依赖于全局学习率RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间适合处理非平稳目标 - 对于RNN效果很好 AdamAdam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下： 其中，，分别是对梯度的一阶矩估计和二阶矩估计，可以看作对期望，的估计；，是对，的校正，这样可以近似为对期望的无偏估计。 可以看出，直接对梯度的矩估计对内存没有额外的要求，而且可以根据梯度进行动态调整，而对学习率形成一个动态约束，而且有明确的范围。 特点： 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点 对内存需求较小 为不同的参数计算不同的自适应学习率 也适用于大多非凸优化 - 适用于大数据集和高维空间 AdamaxAdamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。公式上的变化如下： 可以看出，Adamax学习率的边界范围更简单 NadamNadam类似于带有Nesterov动量项的Adam。公式如下： 可以看出，Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果。 经验之谈 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值 SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。 Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果 最后展示两张可厉害的图，一切尽在图中啊，上面的都没啥用了… …损失平面等高线 在鞍点处的比较 转载须全文转载且注明作者和原文链接，否则保留维权权利 引用 [1]Adagrad[2]RMSprop[Lecture 6e][3]Adadelta[4]Adam[5]Nadam[6]On the importance of initialization and momentum in deep learning[7]Keras中文文档[8]Alec Radford(图)[9]An overview of gradient descent optimization algorithms[10]Gradient Descent Only Converges to Minimizers[11]Deep Learning:Nature","categories":[],"tags":[]}]}